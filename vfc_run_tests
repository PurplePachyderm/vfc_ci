#!/usr/bin/env python3

# This script reads the vfc_tests_config.json and executes the tests accordingly
# It will also generate a ... .vfcrun.json file with the results of the run

import os
import json
import glob

import sys

import calendar
import time

import pandas as pd
import numpy as np

import base64
import struct



    # Helper functions that reads a probes CSV files

def read_probes_csv(filepath, backend):

    results = pd.read_csv(filepath)

    # NOTE The lambda function should probably change depending on if the system
    # uses little or big endian
    results["value"] = results["value"].apply(lambda x: float.fromhex(x))

    results[["test", "variable"]] = results["key"].str.split(':', expand=True)

    del results["key"]
    results.rename(columns = {"value":"values"}, inplace = True)

    results["vfc_backend"] = backend

    return results



    # Open and read the tests config file

try:
    with open("vfc_tests_config.json", "r") as file:
        data = file.read()

except FileNotFoundError as e:
    e.strerror = "This file is required to describe the tests to run and generate a Verificarlo run file"
    raise e

config = json.loads(data)



    # Parse CLI arguments

is_git_commit = False
export_raw_values = False

for i in range(1, len(sys.argv)):
    if sys.argv[i] == "--is-git-commit" or sys.argv[i] == "-g":
        is_git_commit = True

    if sys.argv[i] == "--export-raw-values" or sys.argv[i] == "-r":
        export_raw_values = True



    # Set up metadata

# Metadata and filename are initiated as if no commit was associated
metadata = {
    "timestamp": calendar.timegm(time.gmtime()),
    "is_git_commit": is_git_commit
}
filename = str(metadata["timestamp"])

if is_git_commit:
    print("Fetching metadata from last commit...")
    from git import Repo

    repo = Repo(".")
    head_commit = repo.head.commit

    metadata["timestamp"] = head_commit.authored_date
    filename = str(head_commit)[0:7]

    metadata["hash"] = str(head_commit)[0:7]
    metadata["author"] = str(head_commit.author) + " <" + head_commit.author.email + ">"
    metadata["message"] = head_commit.message



    # Run the build command

print("Building tests...")
os.system(config["make_command"])


    # Execute all tests and collect results in a Pandas dataframe

values = [] # This is an array of Pandas dataframes for now

for executable in config["executables"]:
    print("Running executable :", executable["executable"], "...")

    parameters = ""
    if "parameters" in executable:
        parameters = executable["parameters"]

    for backend in executable["vfc_backends"]:

        export_backend = 'VFC_BACKENDS="' + backend["name"] + '" '
        command = "./" + executable["executable"] + " " + parameters

        repetitions = 1
        if "repetitions" in backend:
            repetitions = backend["repetitions"]

        # Run test repetitions and save results
        for i in range(repetitions):
            os.system(export_backend + command)

            # List and read all the output files
            output_files = glob.glob(executable['output_files'])

            for file in output_files:
                values.append(read_probes_csv(file, backend["name"]))

    # Clean output files for this executable
    os.system("rm -f " + executable['output_files'])

# Combine all separate executions in one dataframe
values = pd.concat(values, sort=False, ignore_index=True)
values = values.groupby(["test", "vfc_backend", "variable"]).values.apply(list).reset_index()



    # Data processing

print("Processing data...")

def numpy_float_array(x):
    return np.array(x).astype(float)

def get_quantile(array, p):
    return np.quantile(array, p)

values["values"] = values["values"].apply(numpy_float_array)

values["mu"] = values["values"].apply(np.average)
values["sigma"] = values["values"].apply(np.std)

# NOTE : What about s when the sample's avg is 0 ? (divison by 0 returns NaN)
values["s10"] = - np.log10(np.absolute( values["sigma"] / values["mu"] ))
values["s2"] = - np.log2(np.absolute( values["sigma"] / values["mu"] ))

# NOTE : Would sorting values["values"] before getting the quantiles give
# better performances ?
values["quantile10"] = values["values"].apply(get_quantile, args=(0.1,))
values["quantile50"] = values["values"].apply(get_quantile, args=(0.5,))
values["quantile90"] = values["values"].apply(get_quantile, args=(0.9,))

values = values.set_index(["test", "vfc_backend", "variable"])



    # Concatenate data and metadata, then export to hdf5

print("Run successful, exporting results...")

values = pd.concat({json.dumps(metadata): values}, names=['metadata'])

# NOTE : Exporting to HDF5 requires to install "tables" on the system
if export_raw_values:
    values.to_hdf(filename + "_raw.hd5", key="data")

del values["values"]
values.to_hdf(filename + ".hd5", key="data")
